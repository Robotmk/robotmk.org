---
draft: false
title: "Robocon 2026 - Recap (Teil 3)"
# --- Italic subheading
# lead: 
# -- giscus id to match comments
commentid: robocon26-recap-3
# slug: 
# -- for posts in menubar, use this (shorter) title
# menutitle: 
description: null
date: "2026-02-14T10:04:33+02:00"
categories:
  - news
tags:
  - "robocon"
authorbox: true
sidebar: true
pager: false
thumbnail: "img/robocon.png"
---

Dies ist **Teil 3** der sechsteiligen Review der Robocon 2026 in Helsinki.

<!--more-->

---

‚ûõ Zur√ºck zu **[Teil 2]({{< ref "/robocon26-recap-2/" >}})**  
‚ûõ Weiter zu **[Teil 4]({{< ref "/robocon26-recap-4/" >}})**

---

### Kann KI uns helfen, Bugs in Robot Framework schneller zu finden?

{{< portrait src="img/fabian.png" alt="Fabian Streitel" >}}

**Fabian Streitel** ber√§t seit √ºber zehn Jahren seine Kunden im Bereich der Testautomatisierung. Er pr√§sentierte einen faszinierenden Ansatz f√ºr ein Problem, das viele Teams mit gro√üen Testsuites kennen: Wie kann man **m√∂glichst schnelles Feedback** liefern, wenn die vollst√§ndige **Testausf√ºhrung Stunden oder gar Tage** dauert?

Die Kernidee seiner Pr√§sentation: satt die gesamte Testsuite zu durchlaufen, clustert man Tests und w√§hlt die zur Ausf√ºhrung aus, die in einem vektorbasierten Raum m√∂glichst weit voneinander entfernt sind - quasi ein "intelligenter Smoke-Test" üòâ  

![alt text](img/talk-3d.gif)

Auf diese Weise wird verhindert, dass die Testroutinen wiederholt redundante Pfade im Code durchlaufen, w√§hrend andere Bereiche noch ungetestet bleiben.

Fabian zeigte, wie er mittels sogenanntem **Mutation Testing** gezielt hunderte von Bugs in den Robot-Framework-Quellcode (als Testkaninchen) eingebracht hatte ‚Äì ein kontrollierbares Testszenario, um die Effektivit√§t seines Ansatzes zu beweisen.  


---

### Traceable Automation in Space Projects

{{< portrait src="img/bruno.png" alt="Bruno N√©stor Calvo Chevillat" >}}

{{< portrait src="img/jose.png" alt="Jos√© Mar√≠a Mart√≠n Bl√°zquez" >}}

Allein der Titel verfing schon bei mir! ü™ù üòÖ  

In einem hochregulierten Umfeld, wo jeder Fehler katastrophale Folgen haben kann, gelten Anforderungen an Testautomatisierung, die weit √ºber typische Web- oder App-Szenarien hinausgehen.

![alt text](img/talk-gmv.png)

Bruno und Jos√© zeigten, wie sie Robot Framework als zentrales Element ihrer Testautomatisierung etabliert haben, eng verzahnt mit Requirements-Management-Tools wie **IBM DOORS**.  

Die Herausforderung bestand darin, eine **bidirektionale Synchronisation** zwischen Anforderungsdefinitionen, Testprozeduren und deren Implementierung zu schaffen. So kann jeder einzelne automatisierte Testfall direkt auf eine spezifische Anforderung zur√ºckverfolgt werden ‚Äì eine **durchg√§ngige Kette der Nachvollziehbarkeit**, wie sie in derart sicherheitskritischen Systemen wie der Raumfahrt zwingend erforderlich ist.

Die Pr√§sentation beleuchtete dabei nicht nur die technische Integration, sondern auch die organisatorischen Konventionen, die in einem solchen Umfeld nat√ºrlich unverzichtbar sind.  
Gl√ºcklicherweise erf√ºllt Robot Framework die regulatorischen Standards und strikten Vorgaben der Luft- und Raumfahrtbranche f√ºr Dokumentation, Tagging und Reporting.  

Die Sprecher teilten auch offen ihre **Lessons Learned** ‚Äì von Fallstricken bis zu konkreten Empfehlungen f√ºr andere, die Automatisierung in regulierten oder sicherheitskritischen Industrien einf√ºhren m√∂chten. Es war deutlich zu sp√ºren, dass die beiden aus jahrelanger Erfahrung berichteten. 

üëâ **Fazit**: Der Vortrag machte klar, dass die Einfachheit und Erweiterbarkeit von Robot Framework keineswegs auf einfache Szenarien beschr√§nkt ist ‚Äì im Gegenteil.  
Mit der richtigen Disziplin und einem durchdachten Framework l√§sst sich mit Robot Framework auch in den anspruchsvollsten technischen Umgebungen eine robuste, nachvollziehbare Automatisierung aufbauen. Selten bekommt man Einblick in derart sensible, hochsichere Bereiche. 

---

### Keyword-Driven Performance Testing Without Manual Scripting

{{< portrait src="img/rakan.png" alt="Rakan Alrasheed" >}}

{{< portrait src="img/abdulelah.png" alt="Abdulelah Alharabi" >}}

Die beiden Sprecher pr√§sentierten eine innovative Architektur, die ein h√§ufig √ºbersehenes Problem adressiert: die Trennung zwischen funktionalen Tests und Performance-Tests. Ihr Ansatz eliminiert diese L√ºcke, indem er Robot Framework als **"Source of Truth"** f√ºr beide Testszenarien etabliert.

Die Kernidee: Funktionale Testszenarien, die bereits in Robot Framework definiert sind, werden automatisch in [Locust](https://locust.io)-Skripte √ºbersetzt ‚Äì ein leistungsstarkes, Python-basiertes Load-Testing-Tool.  
Was normalerweise manuelles Scripting und spezialisiertes Wissen erfordert, wird hier durch ein keyword-basiertes, intent-getriebenes System ersetzt.

Der Vortrag machte deutlich, dass die Wiederverwendbarkeit von Testdefinitionen ein oft untersch√§tzter Hebel ist.  
Wenn Teams ihre funktionalen Tests als Grundlage f√ºr Performance-Tests nutzen k√∂nnen, entsteht nicht nur Effizienz ‚Äì es entsteht auch eine engere Verzahnung zwischen Qualit√§tssicherung und Performance-Engineering - in modernen Entwicklungszyklen unverzichtbar.

---

### Automated Accessibility for "Very Busy" Teams


{{< portrait src="img/lalit.png" alt="Lalitkumar Bhamare" >}}

{{< portrait src="img/affaf.png" alt="Affaf Malik" >}}

**√úber 90%** (!) der eine Million meistbesuchten Websites weisen **Accessibility-Probleme** auf.  
Das stellt nicht nur ein technisches, sondern auch ein gesch√§ftliches, rechtliches und ethisches Problem dar: Nutzer, die auf assistive Technologien angewiesen sind, sto√üen t√§glich auf Barrieren.   

Das liegt nicht einmal daran, dass Teams das Thema "Accessibility" unbedingt ignorieren wollen. Sondern weil sie schlicht nicht die Kapazit√§t, das Budget oder auch manchmal das spezialisierte Wissen haben, um umfassende manuelle Tests daf√ºr durchzuf√ºhren.

Affaf und Lalitkumar zeigten eine **"Shift-Left"-Strategie** auf (wobei "left" = "fr√ºher"), die Accessibility-Testing **ganz vorn** im Entwicklungszyklus verankert.  
In ihrem Ansatz gliedert sich das in drei Ebenen:

- Auf **Entwicklungsebene** k√∂nnen Probleme bereits erkannt werden, bevor √ºberhaupt automatisierte Tests geschrieben werden. Verst√∂√üe wie etwa fehlende "alt"-Texte oder inkorrekte ARIA-Attribute k√∂nnen die Entwickler direkt beim Coding erkennen und korrigieren. 
- Auf **Testebene** integriert Robot Framework Tools wie [axe-core](https://github.com/dequelabs/axe-core) und  nahtlos in funktionale und Regressionstests. Accessibility-Checks sollen damit Teil des t√§glichen Testings werden. ‚Äì ohne zus√§tzlichen manuellen Aufwand.
- Auf **Prozessebene** werden die Tests in CI/CD-Pipelines eingebunden. Erkannte Issues k√∂nnen automatisch getrackt und mit Development-Tasks verkn√ºpft werden, sodass kontinuierliche Validierung stattfindet und Regressionen vor dem Deployment verhindert werden.

Die zentrale Botschaft der Session war klar: Accessibility-Automatisierung ist nicht nur ein Werkzeug zum Aufsp√ºren von Verst√∂√üen ‚Äì sie verdient ein **nachhaltiges System**, in dem Technologie aktiv Diversit√§t und Nutzbarkeit unterst√ºtzt.  

Aber auch die Kehrseite beleuchteten die beiden: "*accessibility can backfire*", wenn sie falsch implementiert wird oder wenn automatisierte Checks ein falsches Sicherheitsgef√ºhl vermitteln, ohne die tats√§chliche Nutzererfahrung zu ber√ºcksichtigen.  
Allzu leichtfertig wird das Thema n√§mlich einfach nur abgehakt - und Jahre sp√§ter kann sich kaum einmal mehr jemand an die Rahmenbedingungen erinnern. 

---

‚ûõ Zur√ºck zu [Teil 2]({{< ref "/robocon26-recap-2/" >}})  
‚ûõ Weiter zu [Teil 4]({{< ref "/robocon26-recap-4/" >}})
